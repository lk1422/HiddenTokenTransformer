import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class clippedLossSequential(nn.Module):

    def __init__(self, eps, c_1, c_2, pad_idx, device):
        """
        Parameters:
            eps      : clip range parameter clip(x, 1-eps, 1+eps)
            c_1      : weight of MSE loss to total loss
            c_2      : weight of entropy loss
            device   : cpu / gpu
        """
        super().__init__()
        self.eps = eps
        self.c_1 = c_1
        self.c_2 = c_2
        self.pad_idx = pad_idx
        self.device = device

    def get_gae(self, rewards, eos_mask):
        reward = rewards*eos_mask
        return torch.cumsum(reward.flip(dims=[1]), dim=1).flip(dims=[1])

    def forward(self, model, model_old, src, generated, actions, rewards, eos_mask):
        """
        Parameters:
            model    : current model for evaluating the policy
            model_old: previous model for evaluating the policy
            src      : src sequence
            generated: sequence generated by current policy
            actions  : Sequence of actions taken to generate generated
            rewards  : sum of rewards total for the episode (Assumes one reward at end of seq)
            eos_mask : mask of time points after the sequence has ended (False for t > T)
        """

        generated = generated[:, :-1]

        model.eval()
        model_old.eval()

        N = generated.shape[0]
        S = generated.shape[1]
        eos_mask_trimmed = eos_mask[:, :-1]

        policy, value = model(src, generated, self.pad_idx, value=True)
        policy_old    = model_old(src, generated, self.pad_idx)
        policy = F.softmax(policy, dim=-1)
        policy_old = F.softmax(policy_old, dim=-1)

        entropy = -(eos_mask_trimmed.unsqueeze(-1) * policy*torch.log(policy)).sum(dim=-1)
        entropy_loss = 1/(N*(eos_mask_trimmed.sum())) * entropy.sum()

        """
        print("VALUES")
        print(value)

        print("POLICIES")
        print(policy)
        print(policy_old)

        print("ENTROPY")
        print(entropy)
        print(entropy_loss)
        """

        batch_index = torch.arange(N).unsqueeze(1).expand(N,S).to(self.device)
        state_index = torch.arange(S).expand(N,S).to(self.device)

        pi = policy[batch_index, state_index, actions]
        pi_old = policy_old[batch_index, state_index, actions]

        r = torch.exp(torch.log(pi) - torch.log(pi_old))
        r_clipped = torch.clamp(r, min=1-self.eps, max=1+self.eps)


        gae = self.get_gae(rewards, eos_mask_trimmed).to(torch.float32)
        value = value.squeeze(-1)
        A = (gae - value)

        l_clip = (torch.min(r, r_clipped) * A) * eos_mask_trimmed
        l_clip = l_clip.sum()/(N*eos_mask_trimmed.sum())

        gae = gae * eos_mask_trimmed
        value = value * eos_mask_trimmed
        gae = gae.reshape(-1,1) 
        value   = value.reshape(-1,1)

        """
        print("GAE")
        print(gae)
        print("VALUE")
        print(value)
        """

        l_value = ( F.mse_loss(value, gae))

        model.train()
        model_old.train()

        """
        print("CLIP")
        print(l_clip)
        print("V")
        print(l_value)
        print("Entropy")
        print(entropy_loss)
        """
        #print(l_value)
        
        return -l_clip + self.c_1*l_value - self.c_2 * entropy_loss, l_value.item()



