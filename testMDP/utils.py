import numpy as np
import copy
import torch
import torch.nn.functional as F
from torch.distributions.categorical import Categorical

def softmax(x):
    exp = np.exp(x)
    return exp / np.sum(exp, axis=0, keepdims=True)

def generate_sequence_grid_world(model, gw, batches, device):
    """Generate sequence of states generated by the model's policy"""

    model.eval()
    max_iterations = gw.states


    valid_states = gw.get_non_terminal()
    x = torch.tensor(np.random.choice(valid_states, size=(batches, 1))).to(device)

    generated_actions = []
    tgt = [x]
    rewards = []
    acts = []
    completed = torch.zeros(batches).to(device)

    for i in range(gw.states):
    #for i in range(4):
        tgt_in = torch.cat(tgt, dim=1).to(torch.int32)
        policy = F.softmax(model(x, tgt_in, -1), dim=-1)[:,-1,:]
        distrib = Categorical(policy)
        actions = distrib.sample()
        acts.append(actions)
        actions= actions.cpu().numpy()
        state_mdp = x.cpu().numpy()
        if len(tgt) > 1:
            state_mdp = tgt[-1].cpu().numpy()

        #print(state_mdp)

        x_next, r, t = gw.batched_step(state_mdp, actions)
        x_next = torch.tensor(x_next).to(device).unsqueeze(1)

        rewards.append(torch.tensor(r).to(device))
        tgt.append(x_next)

        if (t.all()): break

    tgt = torch.cat(tgt, dim=1).to(torch.int32)
    rewards = torch.stack(rewards, dim=1)
    acts = torch.stack(acts, dim=1)
    return x, tgt, rewards, acts

def generate_sequence(model, mdp, batches):
    """This is intended for the basic MDP                         """
    """Generate sequence of states generated by the model's policy"""
    model.eval()
    x = torch.tensor(mdp.get_start_state(batches)).to(device)
    generated_actions =  None
    tgt = None
    rewards = torch.zeros(batches)
    for i in range(5):
        policy = model(x, tgt)
        actions = torch.argmax(policy, dim=-1).detach().cpu()[:, -1]
        if generated_actions is None:
            generated_actions = actions.unsqueeze(1)
        else:
            generated_actions = torch.concat((generated_actions, actions.unsqueeze(1)), dim=1)
        actions = actions.numpy()
        state_numpy = x.detach().cpu().numpy()

        if tgt is not None:
            tgt_numpy = tgt.detach().cpu().numpy()
            state_numpy = tgt_numpy[:, -1, :]
        else:
            state_numpy = state_numpy.reshape((state_numpy.shape[0], state_numpy.shape[2]))

        x_next, r, _ = mdp.batched_step(state_numpy, actions)

        rewards = rewards + r

        x_next_torch = torch.tensor(x_next).to(device).unsqueeze(1)

        if tgt is None:
            tgt = x_next_torch
        else:
            tgt = torch.concat((tgt, x_next_torch), dim=1)

        model.train()

    return x, tgt, generated_actions.to(device), rewards.to(device)

def trainPPO(model, device, mdp, epochs, sub_epochs, batch_size, loss, optim):
    model_old = copy.deepcopy(model)
    losses = []
    rewards = []
    success = []
    for e in range(epochs):

        src, gen, reward, acts = generate_sequence_grid_world(model, mdp, batch_size, device)

        eos_mask = get_eos_mask(mdp, gen).to(device)
        eos_mask_trimmed = eos_mask[:, :-1]

        d = torch.tensor(mdp.destination).to(device)
        batch_success = (d[gen].sum(dim=-1) != 0)

        prec_success = batch_success.sum() / batch_size
        success.append(prec_success)
        print("% OF DEST", round(prec_success.item(), 5))

        temp_model = copy.deepcopy(model)
        avg_loss = 0
        avg_MSE_loss = 0
        for s_e in range(sub_epochs):
            optim.zero_grad()
            l, l_f = loss(model, model_old, src, gen, acts, reward, eos_mask)
            l.backward()
            optim.step()
            avg_loss+=l.item()
            avg_MSE_loss += l_f

        avg_loss = avg_loss/sub_epochs
        avg_MSE_loss = avg_MSE_loss / sub_epochs
        losses.append(avg_loss)
        avg_reward = (reward*eos_mask_trimmed).sum()/batch_size
        rewards.append(avg_reward.item())
        print("EPOCH", e+1)
        print("Average Loss", avg_loss)
        print("Average MSE Loss", avg_MSE_loss)
        print("Average Reward", avg_reward)
        print("="*10)

        model_old = temp_model
    torch.save(model.state_dict(), "temp_model.pth")
    return losses, rewards, success

def get_eos_mask(gw, states):
    states = states.cpu().numpy().astype(np.int32)
    states[:, 0] = gw.start
    eos = gw.terminal[states] | gw.destination[states]
    eos = torch.tensor(eos)
    eos_mask = eos
    #first_eos = (eos.cumsum(dim=1) == 1)
    #eos_mask = torch.logical_xor(eos, first_eos)
    eos_mask = torch.logical_not(eos_mask)
    return eos_mask
